{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFG_procesamiento.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVwU1fJXgZHG",
        "outputId": "46a27bd7-35fa-4992-a0d2-61624299544e"
      },
      "source": [
        "!python -m spacy download es_core_news_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting es_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.2.5/es_core_news_sm-2.2.5.tar.gz (16.2MB)\n",
            "\u001b[K     |████████████████████████████████| 16.2MB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from es_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (57.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (4.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.4.1)\n",
            "Building wheels for collected packages: es-core-news-sm\n",
            "  Building wheel for es-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for es-core-news-sm: filename=es_core_news_sm-2.2.5-cp37-none-any.whl size=16172935 sha256=b41abb36b7bd584c1ffd9b3deef847273daa52039098fce26ed04f5ba148ad9d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-y1tfxrka/wheels/05/4f/66/9d0c806f86de08e8645d67996798c49e1512f9c3a250d74242\n",
            "Successfully built es-core-news-sm\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('es_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b99KkSE13lST",
        "outputId": "b8437f88-b888-4761-aaab-c2532a1be01e"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "import spacy\n",
        "import es_core_news_sm\n",
        "\n",
        "import codecs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "3NPnIQeR30Ij",
        "outputId": "44a43eb2-a505-4312-ebdd-4b6b239f6fa5"
      },
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = '/content/drive/MyDrive/TFG/textos.csv'\n",
        "\n",
        "# Leer csv donde se encuentran el conjunto de textos de los distintos ODS, cada uno de los atributos esta delimitado por ;; \n",
        "df = pd.read_csv(path, sep=\";;\", engine='python', names=['Numero texto', 'ODS', 'Texto'])\n",
        "\n",
        "# Poner los ODS con tipo int \n",
        "for i in range(len(df.index)):\n",
        "  df['ODS'][i] = df['ODS'][i][3:]\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Numero texto</th>\n",
              "      <th>ODS</th>\n",
              "      <th>Texto</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1001</td>\n",
              "      <td>1</td>\n",
              "      <td>﻿﻿﻿﻿EROSKI y sus clientes han logrado canaliz...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1002</td>\n",
              "      <td>1</td>\n",
              "      <td>﻿﻿﻿﻿ El Corte Inglés ha recogido en 2020 casi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1003</td>\n",
              "      <td>1</td>\n",
              "      <td>﻿﻿﻿﻿Fundación Solidaridad Carrefour ha presen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1004</td>\n",
              "      <td>1</td>\n",
              "      <td>﻿﻿﻿﻿La cadena de hoteles RIU se ha unido una ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1005</td>\n",
              "      <td>1</td>\n",
              "      <td>﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿Orange pondrá en marcha el próxim...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9949</th>\n",
              "      <td>10953</td>\n",
              "      <td>17</td>\n",
              "      <td>Durante el Thales iDay, se llevó a cabo una m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9950</th>\n",
              "      <td>10954</td>\n",
              "      <td>17</td>\n",
              "      <td>2016 ha sido un año repleto de desafíos para ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9951</th>\n",
              "      <td>10955</td>\n",
              "      <td>17</td>\n",
              "      <td>Con esta firma SUPRACAFÉ se compromete a impl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9952</th>\n",
              "      <td>10956</td>\n",
              "      <td>17</td>\n",
              "      <td>En la página web del Congreso ya está abierto...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9953</th>\n",
              "      <td>10957</td>\n",
              "      <td>17</td>\n",
              "      <td>Este año InGoodCompanies se centraba en proye...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9954 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Numero texto ODS                                              Texto\n",
              "0             1001   1   ﻿﻿﻿﻿EROSKI y sus clientes han logrado canaliz...\n",
              "1             1002   1   ﻿﻿﻿﻿ El Corte Inglés ha recogido en 2020 casi...\n",
              "2             1003   1   ﻿﻿﻿﻿Fundación Solidaridad Carrefour ha presen...\n",
              "3             1004   1   ﻿﻿﻿﻿La cadena de hoteles RIU se ha unido una ...\n",
              "4             1005   1   ﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿Orange pondrá en marcha el próxim...\n",
              "...            ...  ..                                                ...\n",
              "9949         10953  17   Durante el Thales iDay, se llevó a cabo una m...\n",
              "9950         10954  17   2016 ha sido un año repleto de desafíos para ...\n",
              "9951         10955  17   Con esta firma SUPRACAFÉ se compromete a impl...\n",
              "9952         10956  17   En la página web del Congreso ya está abierto...\n",
              "9953         10957  17   Este año InGoodCompanies se centraba en proye...\n",
              "\n",
              "[9954 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t06-5i1G4Cbj"
      },
      "source": [
        "# Función que elimina los caracteres que tienen algunos textos del csv al principio \n",
        "def clear_first_token(token):\n",
        "  result = \"\"\n",
        "  char = '\\ufeff'\n",
        "  for i in range(len(token)):\n",
        "    if token[i] != char: result += token[i]\n",
        "  return result\n",
        "\n",
        "def preprocess(text):\n",
        "  # Texto en minusculas\n",
        "  text_lower = text.lower()\n",
        "  # Tokenize \n",
        "  token_word = nltk.word_tokenize(text_lower, \"spanish\")\n",
        "  # Algunos textos tienen caracteres raros al principio, por tanto hay que eliminarlos para que no influyan en el código \n",
        "  print(token_word)\n",
        "  token_word[0] = clear_first_token(token_word[0])\n",
        "  # Stopwords de palabras españolas\n",
        "  stopword_spanish = stopwords.words(\"spanish\")\n",
        "  i = 0\n",
        "\n",
        "  while(i < len(token_word)):\n",
        "    # Se eliminan los tokens que se encuentren dentro de las stopwords \n",
        "    if token_word[i] in stopword_spanish:\n",
        "      token_word.remove(token_word[i])\n",
        "    #Se elimina cualquier token que sea distinto de caracteres alfanumericos\n",
        "    elif not (token_word[i].isalpha()):\n",
        "      token_word.remove(token_word[i])\n",
        "    else:\n",
        "      # Debido a que la libreria nltk no lematiza textos en español es necesario utilizar otra libreria de NLP la cual es Spacy\n",
        "      word = sp(token_word[i])[0]\n",
        "      token_word[i] = word.lemma_\n",
        "      i = i+1\n",
        "  return token_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4zLcCO6o4lZC"
      },
      "source": [
        "# Tokenize de todos los textos del csv \n",
        "sp = es_core_news_sm.load()\n",
        "df['Tokens']=df['Texto'].apply(preprocess)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "888cwb8e83Ck"
      },
      "source": [
        "# Se guardan los textos tokenizados en otro csv \n",
        "df\n",
        "df.to_csv(\"textos_procesados.csv\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}