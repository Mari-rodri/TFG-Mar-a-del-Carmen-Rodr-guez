{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFG.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVwU1fJXgZHG",
        "outputId": "0b360e14-e414-4e47-dd06-6085b2559af7"
      },
      "source": [
        "!python -m spacy download es_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b99KkSE13lST",
        "outputId": "89cc2d3d-aa5c-467e-8844-ba912be65a35"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "import spacy\n",
        "import es_core_news_sm\n",
        "\n",
        "import codecs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "3NPnIQeR30Ij",
        "outputId": "b00e379d-436d-4c22-996d-a3880c3903ba"
      },
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = '/content/drive/MyDrive/TFG/textos-reducidos.csv'\n",
        "\n",
        "# Leer csv donde se encuentran el conjunto de textos de los distintos ODS, cada uno de los atributos esta delimitado por ;; \n",
        "df = pd.read_csv(path, sep=\";;\", engine='python', names=['Numero texto', 'ODS', 'Texto'])\n",
        "# Poner los ODS con tipo int \n",
        "for i in range(len(df.index)):\n",
        "  df['ODS'][i] = df['ODS'][i][3:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t06-5i1G4Cbj"
      },
      "source": [
        "# Función que elimina los caracteres que tienen algunos textos del csv al principio \n",
        "def clear_first_token(token):\n",
        "  result = \"\"\n",
        "  char = '\\ufeff'\n",
        "  for i in range(len(token)):\n",
        "    if token[i] != char: result += token[i]\n",
        "  return result\n",
        "\n",
        "def preprocess(text):\n",
        "  # Texto en minusculas\n",
        "  text_lower = text.lower()\n",
        "  # Tokenize \n",
        "  token_word = nltk.word_tokenize(text_lower, \"spanish\")\n",
        "  # Algunos textos tienen caracteres raros al principio, por tanto hay que eliminarlos para que no influyan en el código \n",
        "  token_word[0] = clear_first_token(token_word[0])\n",
        "  # Stopwords de palabras españolas\n",
        "  stopword_spanish = stopwords.words(\"spanish\")\n",
        "  i = 0\n",
        "\n",
        "  while(i < len(token_word)):\n",
        "    # Se eliminan los tokens que se encuentren dentro de las stopwords \n",
        "    if token_word[i] in stopword_spanish:\n",
        "      token_word.remove(token_word[i])\n",
        "    #Se elimina cualquier token que sea distinto de caracteres alfanumericos\n",
        "    elif not (token_word[i].isalpha()):\n",
        "      token_word.remove(token_word[i])\n",
        "    else:\n",
        "      # Debido a que la libreria nltk no lematiza textos en español es necesario utilizar otra libreria de NLP la cual es Spacy\n",
        "      word = sp(token_word[i])[0]\n",
        "      token_word[i] = word.lemma_\n",
        "      i = i+1\n",
        "  return token_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zLcCO6o4lZC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "f015e42b-ea5b-4292-eacf-5baf2b6477d5"
      },
      "source": [
        "# Tokenize de todos los textos del csv \n",
        "sp = es_core_news_sm.load()\n",
        "df['Tokens']=df['Texto'].apply(preprocess)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "888cwb8e83Ck"
      },
      "source": [
        "# Se guardan los textos tokenizados en otro csv \n",
        "df\n",
        "df.to_csv(\"textos_procesados.csv\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3apFmxnqB7t"
      },
      "source": [
        "# Unión de los dos csv \n",
        "# Leer csv donde se encuentran el conjunto de textos de los distintos ODS, cada uno de los atributos esta delimitado por ;; \n",
        "\n",
        "path1 = '/content/drive/MyDrive/TFG/textos_procesados1.csv'\n",
        "df_procesado = pd.read_csv(path1, usecols=['ODS', 'Texto', 'Tokens'])\n",
        "\n",
        "path2 = '/content/drive/MyDrive/TFG/textos_procesados2.csv'\n",
        "df_procesado2 = pd.read_csv(path2, usecols=['ODS', 'Texto', 'Tokens'])\n",
        "\n",
        "df_final = pd.concat([df_procesado, df_procesado2])\n",
        "df_final = df_final.reset_index(drop=True)\n",
        "df_final\n",
        "df_final.to_csv(\"textos_procesados_bien.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}